{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python-projects\\Kaggle\\Foursquare_Location_Matching\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import os\n",
    "import string\n",
    "import random as rnd\n",
    "import Levenshtein\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import lightgbm as lgb\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag to force to reload dataset\n",
    "RELOAD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foursquare-location-matching.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "# initialize Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# download dataset from Kaggle to data folder\n",
    "data_path = 'data'\n",
    "api.competition_download_files('foursquare-location-matching', data_path, force=RELOAD, quiet=False)\n",
    "# save filename: !ATTENTION! : it may not be wroking if many files are in folders\n",
    "# then just name it manually \n",
    "dataset_file_name = 'foursquare-location-matching.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train dataset (train.csv) to pandas DataFrame named df: it will be used for analysis\n",
    "df = pd.read_csv(ZipFile(os.path.join(data_path, dataset_file_name)).open('train.csv'))\n",
    "\n",
    "df_pairs = pd.read_csv(ZipFile(os.path.join(data_path, dataset_file_name)).open('pairs.csv'))\n",
    "\n",
    "# Read test dataset (test.csv), to pandas DataFrame named df_validation. It will be used only to generate final predictions, which will be submitted\n",
    "df_validation = pd.read_csv(ZipFile(os.path.join(data_path, dataset_file_name)).open('test.csv'))\n",
    "# finally, we will download example of submission (there are no correct predictions there, it is just an example)\n",
    "df_subm_example = pd.read_csv(ZipFile(os.path.join(data_path, dataset_file_name)).open('sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2022\n",
    "num_neighbors = 20\n",
    "num_split = 5\n",
    "feat_columns = ['name', 'address', 'city', \n",
    "            'state', 'zip', 'url', \n",
    "           'phone', 'categories', 'country']\n",
    "\n",
    "vec_columns = ['name', 'address', 'city', 'state', 'zip', 'country', 'categories']\n",
    "\n",
    "def seed_everything(seed):\n",
    "    rnd.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df: (1138812, 13)\n",
      "Shape of df_pairs: (578907, 25)\n",
      "Shape of df_validation: (5, 12)\n",
      "Shape of df_subm_example: (5, 2)\n"
     ]
    }
   ],
   "source": [
    "# Check, that all dataframes are loaded and have correct shapes\n",
    "print(f'Shape of df: {str(df.shape)}')\n",
    "print(f'Shape of df_pairs: {str(df_pairs.shape)}')\n",
    "print(f'Shape of df_validation: {str(df_validation.shape)}')\n",
    "print(f'Shape of df_subm_example: {str(df_subm_example.shape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text):\n",
    "    # zip field, sometimes is read as float\n",
    "    if not isinstance(text,str):\n",
    "        text = str(int(text))\n",
    "    text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    text = text.lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_drop=['phone', 'url']):\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.drop(self.columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanString(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, coulums_to_clean=['name', 'address', 'city', 'state', 'zip', 'country', 'categories']):\n",
    "        self.columns_to_clean = coulums_to_clean\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        for col in self.columns_to_clean:\n",
    "            X[f'{col}_clean']=X[col].map(clean_string, na_action='ignore')\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_knn(df, Neighbors = 5):   \n",
    "    neighbors = min(len(df), Neighbors)\n",
    "    train_df = []\n",
    "    knn = NearestNeighbors(n_neighbors = neighbors)\n",
    "    knn.fit(df[['latitude','longitude']])\n",
    "    dists, nears = knn.kneighbors(df[['latitude','longitude']])\n",
    "    \n",
    "    for k in range(neighbors):            \n",
    "        cur_df = df[['id']].copy()\n",
    "        cur_df['match_id'] = df['id'].values[nears[:, k]]\n",
    "        cur_df['kdist'] = dists[:, k]\n",
    "        cur_df['kneighbors'] = k\n",
    "        train_df.append(cur_df)\n",
    "    \n",
    "    train_df = pd.concat(train_df)\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Knn_geo(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = recall_knn(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_records(df):  \n",
    "    \n",
    "    df_knn = recall_knn(df)\n",
    "      \n",
    "    merged_df = df_knn.merge(df, how='inner', left_on='id', right_on='id')\n",
    "    df_pairs_custom = merged_df.merge(df, how='inner', left_on='match_id', right_on='id')\n",
    "    df_pairs_custom.drop(['match_id'], axis=1, inplace=True)\n",
    "    # if it is train set - create match column\n",
    "    train = ('point_of_interest' in df.columns)\n",
    "    if train:\n",
    "        df_pairs_custom['match'] = df_pairs_custom['point_of_interest_x'] == df_pairs_custom['point_of_interest_y']\n",
    "        df_pairs_custom.drop(['point_of_interest_x', 'point_of_interest_y'], axis=1, inplace=True)\n",
    "\n",
    "    # df_pairs_custom.drop(['name_clean_x', 'address_clean_x', 'city_clean_x', 'state_clean_x', 'zip_clean_x', 'country_clean_x', 'categories_clean_x',\n",
    "    #                         'name_clean_y', 'address_clean_y', 'city_clean_y', 'state_clean_y', 'zip_clean_y', 'country_clean_y', 'categories_clean_y'], axis=1, inplace=True)\n",
    "\n",
    "    # columns = ['id_1', 'geo_k_dist', 'geo_k_neigh', 'latitude_1', 'longitude_1', 'name_1', 'address_1', 'city_1', 'state_1', 'zip_1', 'country_1', 'categories_1',\n",
    "    #                                 'name_vec_1', 'address_vec_1', 'city_vec_1', 'state_vec_1', 'zip_vec_1', 'country_vec_1', 'categories_vec_1',\n",
    "    #                                 'id_2', 'latitude_2', 'longitude_2', 'name_2', 'address_2', 'city_2', 'state_2', 'zip_2', 'country_2', 'categories_2',\n",
    "    #                                 'name_vec_2', 'address_vec_2', 'city_vec_2', 'state_vec_2', 'zip_vec_2', 'country_vec_2', 'categories_vec_2']\n",
    "\n",
    "    columns = ['id_1', 'geo_k_dist', 'geo_k_neigh', 'latitude_1', 'longitude_1', 'name_1', 'address_1', 'city_1', 'state_1', 'zip_1', 'country_1', 'categories_1',\n",
    "                                    'id_2', 'latitude_2', 'longitude_2', 'name_2', 'address_2', 'city_2', 'state_2', 'zip_2', 'country_2', 'categories_2']\n",
    "\n",
    "    if train: columns.append('match')\n",
    "    df_pairs_custom.columns=columns\n",
    "\n",
    "    return df_pairs_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinePairs(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = combine_records(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "def LCS(str S, str T):\n",
    "    cdef int i, j\n",
    "    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n",
    "    for i in range(len(S)):\n",
    "        for j in range(len(T)):\n",
    "            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n",
    "    return dp[len(S)][len(T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim (vec1, vec2):\n",
    "    vec1 = vec1.reshape(1, -1)\n",
    "    vec2 = vec2.reshape(1, -1)\n",
    "\n",
    "    return cosine_similarity(vec1, vec2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df, str_cols=['name', 'address', 'city', 'state', 'zip', 'country', 'categories']):\n",
    "    df_new = df.copy()\n",
    "    for col in str_cols:\n",
    "        # Add string distances to df\n",
    "        df_new[f'{col}_lev'] = df_new.apply(lambda x: Levenshtein.distance(str(x[f'{col}_1']), str(x[f'{col}_2'])), axis=1)\n",
    "        df_new[f'{col}_jaro'] = df_new.apply(lambda x: Levenshtein.jaro_winkler(str(x[f'{col}_1']), str(x[f'{col}_2'])), axis=1)\n",
    "        df_new[f'{col}_seq_match'] = df_new.apply(lambda x: difflib.SequenceMatcher(None, str(x[f'{col}_1']), str(x[f'{col}_2'])).ratio(), axis=1)\n",
    "        df_new[f'{col}_lcs'] = df_new.apply(lambda x: LCS(str(x[f'{col}_1']), str(x[f'{col}_2'])), axis=1)\n",
    "\n",
    "        # Vector distances\n",
    "        #df_new[f'{col}_cos_sim'] = df_new.apply(lambda x: cosine_sim(x[f'{col}_vec_1'], x[f'{col}_vec_2']), axis=1)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    # df_new.drop(['latitude_1', 'longitude_1', 'name_1', 'address_1', 'city_1', 'state_1', 'zip_1', 'country_1', 'categories_1', \n",
    "    #             'name_vec_1', 'address_vec_1', 'city_vec_1', 'state_vec_1', 'zip_vec_1', 'country_vec_1', 'categories_vec_1', \n",
    "    #             'latitude_2', 'longitude_2', 'name_2', 'address_2', 'city_2', 'state_2', 'zip_2', 'country_2', 'categories_2', \n",
    "    #             'name_vec_2', 'address_vec_2', 'city_vec_2', 'state_vec_2', 'zip_vec_2', 'country_vec_2', 'categories_vec_2'], axis=1, inplace=True)\n",
    "\n",
    "    df_new.drop(['latitude_1', 'longitude_1', 'name_1', 'address_1', 'city_1', 'state_1', 'zip_1', 'country_1', 'categories_1',  \n",
    "                'latitude_2', 'longitude_2', 'name_2', 'address_2', 'city_2', 'state_2', 'zip_2', 'country_2', 'categories_2'], axis=1, inplace=True)                \n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddFeatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = add_features(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('dropper1', ColDropper()),\n",
    "    ('cleaner', CleanString()),\n",
    "    #('vector', VecString()),\n",
    "    ('dropper2', ColDropper(columns_to_drop=['name', 'address', 'city', 'state', 'zip', 'country', 'categories'])),\n",
    "    ('combinator', CombinePairs()),\n",
    "    ('add_features', AddFeatures())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x2de1c0b6140>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_model = CatBoostClassifier()\n",
    "cat_model.load_model('catboost2406')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep = pipe.fit_transform(df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep_no_ids = df_prep.drop(['id_1', 'id_2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = load(open('scaler.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for 'match' column and remove it if exist (for testing I use test dataset, which creates this column)\n",
    "if ('match' in df_prep_no_ids.columns):\n",
    "    df_prep_no_ids.drop('match', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep_no_ids_scaled = scaler.transform(df_prep_no_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cat_model.predict(df_prep_no_ids_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep['match'] = (y_pred == 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_prep[df_prep['match']][['id_1', 'id_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subm = df_out.groupby('id_1')['id_2'].apply(list).reset_index(name='matches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lst): \n",
    "    return ' '.join(lst)\n",
    "\n",
    "df_subm['match_id'] = df_subm['matches'].apply(lambda x: convert(x))\n",
    "df_subm.drop('matches', inplace=True, axis=1)\n",
    "\n",
    "df_subm.columns = ['id', 'matches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subm.to_csv('.\\\\data\\\\test_my_submission.csv', index=False, doublequote=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3abc7ea86c1ba1384b4878b98af82991463d4da0cba7b7e72c3eaf31f3854092"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
